* Consider refactoring the NFA representation such that it can be instantly
loaded from a `&[u8]`, just like a sparse DFA. Main downside is that this could
negatively impact using the NFA with deserialization costs. Before doing this,
we should write PikeVM and backtracking implementations so that they can be
benchmarked.

* Investigate why NFA shrinking seems to produce bigger DFAs after
determinization, even though it makes determinization substantially faster.
This might be because of its use of sparse NFA states, which have a lower
constant overhead associated with them.
  * IDEA: Maybe the "shrinking" part of this isn't really helping as much as
  I think, and it really is just the sparse states. So... Is there a way to
  use the faster shrinking strategy while making use of sparse states? That
  could be a really huge win.

* Scrutinize whether the SparseTransitions and Transition types should have
their representations expose. The State enum is already totally exposed...
  * Yup, expose them.

* Provide a more elementary API for 'start_state_forward'.
See: https://github.com/rust-lang/regex/issues/852
  * Calling this fixed via 'universal_start_state'.

* There is an issue where the minimum cache capacity required by the lazy DFA
is likely too big. This in turn could result in NOT using the lazy DFA even
if it would be okay. We should either fix the minimum to be closer to the
actual minimum or maybe not require a minimum? (The problem is that I think the
minimum is there to avoid runtime panics in degenerate cases...)
  * Calling this fixed. I did two things. 1) I fixed a bug in the minimum
  cache capacity calculation where state heap usage (for NFA IDs) was double
  counters. 2) I also reduced minimum state size from 5 to 4, which is the
  actual real minimum needed (I think). OK, turns out (2) doesn't work because
  5 states is the real minimum.

* There's likely some SIMD we can do to improve parts of the PikeVM.
Specifically, the Sparse state. Maybe PCMPESTI can finally be useful?
  * Actually I'm not sure this makes a lot of sense. The NFA only looks at
  one byte at a time, and PCMPESTRI is about looking at many bytes at a time.
  * This is also difficult because we'd really want PCMPESTRI to get inlined,
  which means most/all of the NFA matching would need to be parameterized over
  whether SIMD is available or not.

* Rename Match to Span, MultiMatch to Match and keep HalfMatch as-is (since
it does include a pattern ID). DONE.

* Prefilters. See top of src/util/prefilter.rs for more in depth thoughts.
  * DONE

* On the topic of prefilters, we should beef up the memchr crate with a few
other things:
  * Expose the memmem prefilter, so that regex engines can use it directly
  without caring whether the full needle matches or not.
  * Move the byteset scanning to memchr, and maybe use PCMPESTRI?
  * Move the is_ascii code?

* I guess make Span impl Copy and probably make it impl Index and IndexMut,
just like we do for grep_matcher::Match.
  * OK, I guess this is strictly done, but do we know modify all of the '*_at'
  APIs to take spans instead of 'start: usize, end: usize'? One big bummer
  here is that code would really benefit from using range syntax in some
  places. We could make that work, but... with generics. Which complicates
  things. Sigh.

* See this discussion about shift DFAs:
  * https://twitter.com/pervognsen/status/1523896138260107264
  * How can we best use them? We should certainly be able to find a place
  for them somewhere. One of the more compelling bits is as a prefilter
  for when literal prefixes can't be found.

* Figure out how to better support Miri. The main issue is that, even in
release mode, our test suite takes quite a bit of time. It's too long even in
debug mode. And Miri is even slower than that. So we clearly can only test
some subset of things. Currently, Miri runs acceptably fast on most doc tests
and all library level tests. But some non-trivial set of doc tests take too
long. And, sadly, none of the TOML test suite tests can run yet. Namely, even
deserializing a single TOML file into memory appears to be incredibly slow.
Ideally, we figure out how to increase Miri test coverage without complicating
things too much.
  * DONE, but not elegantly. Basically just selectively disabled things until
    'cargo miri test' completed in reasonable time.

* Decide and update docs with respect to what happens when you give a search an
invalid range.
  * DONE
